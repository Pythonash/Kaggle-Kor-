{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "확률적 모수 탐색.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pythonash/Kaggle-Kor-/blob/Brain/%ED%99%95%EB%A5%A0%EC%A0%81%20%EB%AA%A8%EC%88%98%20%ED%83%90%EC%83%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이 노트북은 Pythonash에 의해 작성되었습니다.\n",
        "\n",
        "이 노트북의 목적은 여러 파라미터들이 변화할 때 손실 결과 데이터를 수집하는 것에 있습니다.\n",
        "\n",
        "여러 많은 파라미터들의 조합으로, 최적의 파라미터나 어떻게 당신의 모델을 최적화 할 수 있을지 찾을 수 있을 것 입니다.\n",
        "\n",
        "그럼 시작해 볼까요?"
      ],
      "metadata": {
        "id": "KD6gBcwVQS7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요한 라이브러리들을 불러와 줍니다."
      ],
      "metadata": {
        "id": "o9fxtxPj06Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# import ubiquant\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "import time\n",
        "\n",
        "start = time.time()"
      ],
      "metadata": {
        "papermill": {
          "duration": 5.150271,
          "end_time": "2022-02-17T06:49:49.868958",
          "exception": false,
          "start_time": "2022-02-17T06:49:44.718687",
          "status": "completed"
        },
        "tags": [],
        "id": "7IDIXQEyPNZs",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:55:50.439247Z",
          "iopub.execute_input": "2022-03-06T04:55:50.439949Z",
          "iopub.status.idle": "2022-03-06T04:55:55.598092Z",
          "shell.execute_reply.started": "2022-03-06T04:55:50.439851Z",
          "shell.execute_reply": "2022-03-06T04:55:55.597275Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋을 불러오고, 이를 핸들링 합니다.\n",
        "\n",
        "- 데이터를 불러오고 핸들링하는 자세한 내용은 저의 이전 노트북인 [End to end simple and powerful DNN with LeakyReLU](https://www.kaggle.com/pythonash/end-to-end-simple-and-powerful-dnn-with-leakyrelu)에 설명되어 있습니다.\n",
        "\n",
        "- 만약 이에 대해 더 많은 정보가 필요하시다면, 위의 링크를 눌러 확인해 주세요.\n",
        "\n",
        "\n",
        "## 주의\n",
        "\n",
        "이 노트북은 과정이 약간 다릅니다.\n",
        "\n",
        "전의 노트북에서는 \"investment_id\"와 \"표준화\"를 사용하였지만, 이번 노트북 에서는 \"investment_id\"를 사용하지 않으며, 표준화 대신 정규화(Min-Max)를 사용합니다."
      ],
      "metadata": {
        "id": "rj-xHp6Z06L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\n",
        "df"
      ],
      "metadata": {
        "papermill": {
          "duration": 42.185531,
          "end_time": "2022-02-17T06:50:32.211548",
          "exception": false,
          "start_time": "2022-02-17T06:49:50.026017",
          "status": "completed"
        },
        "tags": [],
        "id": "zvgY3iirPNZ0",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:55:55.599696Z",
          "iopub.execute_input": "2022-03-06T04:55:55.600806Z",
          "iopub.status.idle": "2022-03-06T04:56:35.69829Z",
          "shell.execute_reply.started": "2022-03-06T04:55:55.600766Z",
          "shell.execute_reply": "2022-03-06T04:56:35.697549Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_col = df.drop(['row_id','time_id','investment_id','target'],axis=1).columns\n",
        "f_col"
      ],
      "metadata": {
        "papermill": {
          "duration": 2.06051,
          "end_time": "2022-02-17T06:50:36.991602",
          "exception": false,
          "start_time": "2022-02-17T06:50:34.931092",
          "status": "completed"
        },
        "tags": [],
        "id": "7HU8RHAUPNZ2",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:38.057228Z",
          "iopub.execute_input": "2022-03-06T04:56:38.057673Z",
          "iopub.status.idle": "2022-03-06T04:56:40.550998Z",
          "shell.execute_reply.started": "2022-03-06T04:56:38.057605Z",
          "shell.execute_reply": "2022-03-06T04:56:40.550322Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(pd.DataFrame(df[f_col]))"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.095234,
          "end_time": "2022-02-17T06:50:37.176892",
          "exception": false,
          "start_time": "2022-02-17T06:50:37.081658",
          "status": "completed"
        },
        "tags": [],
        "id": "Hl3uby-UPNZ2",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:40.552363Z",
          "iopub.execute_input": "2022-03-06T04:56:40.552789Z",
          "iopub.status.idle": "2022-03-06T04:56:44.415716Z",
          "shell.execute_reply.started": "2022-03-06T04:56:40.552744Z",
          "shell.execute_reply": "2022-03-06T04:56:44.415052Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(df):\n",
        "    f_df = df[f_col]\n",
        "    scaled_f = scaler.transform(pd.DataFrame(f_df))\n",
        "    data_x = pd.DataFrame(scaled_f)\n",
        "    data_x.columns = f_df.columns\n",
        "    del f_df\n",
        "    data_x = data_x.astype('float16')\n",
        "    return data_x"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.037193,
          "end_time": "2022-02-17T06:50:37.308703",
          "exception": false,
          "start_time": "2022-02-17T06:50:37.27151",
          "status": "completed"
        },
        "tags": [],
        "id": "4lRT8668PNZ3",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:44.416927Z",
          "iopub.execute_input": "2022-03-06T04:56:44.417696Z",
          "iopub.status.idle": "2022-03-06T04:56:44.423102Z",
          "shell.execute_reply.started": "2022-03-06T04:56:44.417656Z",
          "shell.execute_reply": "2022-03-06T04:56:44.422258Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df=df.astype('float16')\n",
        "df_x = make_dataset(df)\n",
        "df_x"
      ],
      "metadata": {
        "papermill": {
          "duration": 12.385962,
          "end_time": "2022-02-17T06:50:49.844364",
          "exception": false,
          "start_time": "2022-02-17T06:50:37.458402",
          "status": "completed"
        },
        "tags": [],
        "id": "kdtoVGMLPNZ3",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:44.425971Z",
          "iopub.execute_input": "2022-03-06T04:56:44.426209Z",
          "iopub.status.idle": "2022-03-06T04:56:55.575515Z",
          "shell.execute_reply.started": "2022-03-06T04:56:44.426172Z",
          "shell.execute_reply": "2022-03-06T04:56:55.573649Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y = pd.DataFrame(df['target'])\n",
        "df_y"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.044144,
          "end_time": "2022-02-17T06:50:49.980591",
          "exception": false,
          "start_time": "2022-02-17T06:50:49.936447",
          "status": "completed"
        },
        "tags": [],
        "id": "hfSW4HVfPNZ3",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:55.57674Z",
          "iopub.execute_input": "2022-03-06T04:56:55.577064Z",
          "iopub.status.idle": "2022-03-06T04:56:55.593012Z",
          "shell.execute_reply.started": "2022-03-06T04:56:55.577026Z",
          "shell.execute_reply": "2022-03-06T04:56:55.592346Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.092862,
          "end_time": "2022-02-17T06:50:50.169638",
          "exception": false,
          "start_time": "2022-02-17T06:50:50.076776",
          "status": "completed"
        },
        "tags": [],
        "id": "2vHrSQ3nPNZ4",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:55.594328Z",
          "iopub.execute_input": "2022-03-06T04:56:55.594604Z",
          "iopub.status.idle": "2022-03-06T04:56:55.598252Z",
          "shell.execute_reply.started": "2022-03-06T04:56:55.594568Z",
          "shell.execute_reply": "2022-03-06T04:56:55.597541Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 확률적 모델 구조\n",
        "\n",
        "- 저는 모델을 랜덤하게 구성하였습니다. 즉, 뉴런의 수나 드롭아웃 비율이나 학습률이 매번 다르게 할당될 것 입니다.\n",
        "\n",
        "- LeakyReLU 대신 PReLU를 사용합니다.\n",
        "\n",
        "- 매 반복마다 랜덤한 뉴런과 드롭아웃 비율, 그리고 학습률을 얻으실 겁니다.\n",
        "\n",
        "- 그렇게 함으로써, 파라미터가 변할때 마다 손실 값에 미치는 영향에 대한 기록 데이터를 얻으실 수 있습니다.\n",
        "\n",
        "모델의 구조는 원하시는 만큼 바꿔서 해보실 수 있습니다.\n"
      ],
      "metadata": {
        "id": "obM6qNCI06L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pythonash_model():\n",
        "    neurons = random.randint(16, 513)\n",
        "    drop_rate = random.randint(1,6)/10\n",
        "    lr_rate = random.uniform(0.0005, 0.007)\n",
        "    \n",
        "    inputs_ = tf.keras.Input(shape = [df_x.shape[1]])\n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(inputs_)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    drop = tf.keras.layers.Dropout(drop_rate)(leaky)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(drop)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    drop = tf.keras.layers.Dropout(drop_rate)(leaky)\n",
        "    \n",
        "    outputs_ = tf.keras.layers.Dense(1)(drop)\n",
        "    \n",
        "    model = tf.keras.Model(inputs = inputs_, outputs = outputs_)\n",
        "    \n",
        "    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "\n",
        "    learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate = lr_rate,\n",
        "    decay_steps = 10000,\n",
        "    decay_rate = 0.97)\n",
        "    \n",
        "    adam = tf.keras.optimizers.Adam(learning_rate = learning_sch)\n",
        "    \n",
        "    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n",
        "    opt_name = str(model.optimizer).split('.')[3].split()[0]\n",
        "    print('Current set is \\n neurons: {0},\\n Drop rate: {1}, \\n learning_rate: {2}'.format(neurons, drop_rate, lr_rate))\n",
        "    \n",
        "    return neurons, drop_rate, lr_rate, model"
      ],
      "metadata": {
        "papermill": {
          "duration": 3.3428,
          "end_time": "2022-02-17T06:50:53.606125",
          "exception": false,
          "start_time": "2022-02-17T06:50:50.263325",
          "status": "completed"
        },
        "tags": [],
        "id": "2hKZ3clVPNZ4",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:55.599839Z",
          "iopub.execute_input": "2022-03-06T04:56:55.600584Z",
          "iopub.status.idle": "2022-03-06T04:56:55.617901Z",
          "shell.execute_reply.started": "2022-03-06T04:56:55.600535Z",
          "shell.execute_reply": "2022-03-06T04:56:55.617228Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시뮬레이션 로그 저잠 하기\n",
        "\n",
        "매 반복마다, 손실 값이 계산되는데 이 코드는 자동으로 그것을 저장해 줍니다.\n"
      ],
      "metadata": {
        "id": "Y2cGY3zK06L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_log = []\n",
        "num_iter = 1\n",
        "for iteration in np.arange(1,100):\n",
        "    end = time.time()#\n",
        "    if round(end - start,0) /3600 < 5:#\n",
        "        print('Current running time: {} sec.'.format(round(end - start,0)))\n",
        "        num_fold = 1\n",
        "        kfold_generator = KFold(n_splits =5, shuffle=True)\n",
        "        callbacks = tf.keras.callbacks.ModelCheckpoint('pythonash_model.h5', save_best_only = True)\n",
        "        neurons, drop_rate, lr_rate, model = pythonash_model()\n",
        "        fold_model = model.save('fold_model.h5')\n",
        "        del fold_model\n",
        "        del model\n",
        "        for train_index, val_index in kfold_generator.split(df_x, df_y):\n",
        "            fold_model = tf.keras.models.load_model('fold_model.h5')\n",
        "            # Split training dataset.\n",
        "            train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n",
        "            # Split validation dataset.\n",
        "            val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n",
        "            # Make tensor dataset.\n",
        "            tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n",
        "            tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n",
        "            # Load model\n",
        "            ###############################################################################################        \n",
        "            print('======================================Fold %d Start!======================================='%num_fold)\n",
        "            fit_history = fold_model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n",
        "                     validation_data = (tf_val), shuffle=True, verbose = 1)\n",
        "            min_loss = np.array(fit_history.history['val_loss']).min()\n",
        "            print('===========================================================================================')\n",
        "            print('Model achieves %f in validation set.' %min_loss)\n",
        "            print('===========================================================================================')\n",
        "            simulation_log.append([num_iter, num_fold, neurons, drop_rate, lr_rate, min_loss])\n",
        "            log_df = pd.DataFrame(simulation_log)\n",
        "            log_df.columns = ['num_iter','num_fold','neurons', 'drop_rate', 'lr_rate', 'min_loss']\n",
        "            print(log_df)\n",
        "            log_df.to_csv('./Parameter finder log.csv', encoding = 'utf-8-sig', index = False)\n",
        "            print('===========================================================================================')\n",
        "            # Delete tensor dataset and model for avoiding memory exploring.\n",
        "            del tf_train\n",
        "            del tf_val\n",
        "            del fit_history\n",
        "            del fold_model\n",
        "            num_fold += 1\n",
        "    else:\n",
        "        print('Memory using time is over.')\n",
        "        break\n",
        "#     del model\n",
        "    del neurons\n",
        "    del drop_rate\n",
        "    del lr_rate\n",
        "    del min_loss\n",
        "    print('%d iteraion is over.' %num_iter)\n",
        "    print('===========================================================================================')\n",
        "    num_iter+=1\n",
        "    "
      ],
      "metadata": {
        "id": "j05Pa_AnPNZ5",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:55.619268Z",
          "iopub.execute_input": "2022-03-06T04:56:55.619796Z",
          "iopub.status.idle": "2022-03-06T05:08:37.680288Z",
          "shell.execute_reply.started": "2022-03-06T04:56:55.619756Z",
          "shell.execute_reply": "2022-03-06T05:08:37.679503Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}