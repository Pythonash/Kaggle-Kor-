{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMm5ltN+162C4xI0I+giLXD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pythonash/Kaggle-Kor-/blob/Brain/Untitled56.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glove를 이용한 감성분석 (1) - 내가 가진 데이터 셋으로 학습시키기(영어 데이터셋)\n",
        "\n",
        "1. 파이썬 글로브를 다운로드 받는다.\n",
        "\n",
        "\n",
        "    !pip install glove_python_binary\n",
        "\n",
        "\n",
        "2. 글로브에 적용하기 위한 동시 등장 행렬을 Corpus를 통해, 그리고 이를 glove에 넣어준다.\n",
        "\n",
        "\n",
        "    corpus.fit([total_words], window = 5)\n",
        "\n",
        "- [total_words]: 에러가 안나게끔 리스트로 씌워준 것\n",
        "\n",
        "- window: 각 단어를 기준으로 앞뒤 몇 단어까지 참조할 것인지(DISTANCE)를 고려한다.\n",
        "\n",
        "\n",
        "    glove = Glove(no_components = 100, learning_rate = 0.05)\n",
        "\n",
        "\n",
        "- no_components: 임베딩 이후 몇 차원으로 출력해 줄 것인지 정해주는 파라미터\n",
        "\n",
        "- learning_rate: 말 그대로의 학습률을 지정하는 것\n",
        "\n",
        "\n",
        "    glove.fit(corpus.matrix, epochs = 20, no_threads = 4, verbose = True)\n",
        "\n",
        "\n",
        "- no_threads: 학습시 가동할 자원의 갯수를 말하는 것 같다.\n",
        "\n",
        "\n",
        "    glove.add_dictionary(corpus.dictionary)\n",
        "\n",
        "\n",
        "- .add_dictionary(corpus.dictionary): glove사전에 corpus로부터 학습한 딕셔너리를 추가해준다.\n"
      ],
      "metadata": {
        "id": "JnIqkJMxHNLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torchtext (ver. new)를 이용한 데이터셋 준비\n",
        "\n",
        "가장 최근에 배포된 것으로 이걸 이용해서 감성분석을 하는 태스크를 진행해본다.\n",
        "\n",
        "하지만, 현재 Beta버젼인게 많기 때문에 언제든 바뀔 수 있다.\n",
        "\n",
        "그때는 공식문서인 [이곳](https://pytorch.org/text)로 들어가서 \b관련 문서를 살펴보면 된다.\n",
        "\n",
        "torchtext 데이터셋을 다운로드 하기 위해서 `torchdata`가 반드시 필요하다.\n",
        "\n",
        "또한, 설치 후 exit()로 런타임을 다시 돌려야 정상적으로 돌아간다.\n",
        "\n",
        "- [UPDATED] 2022.11.01\n",
        "\n",
        "torchtext 새로운 버젼이 릴리즈 되고 나서 0.14.0 버젼을 깔아주어야만 다음과 같이 정상적으로 작동한다."
      ],
      "metadata": {
        "id": "EiW1GdmFHkmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata\n",
        "!pip install torchtext==0.14.0\n",
        "from google.colab import output as ot\n",
        "ot.clear()\n",
        "exit()"
      ],
      "metadata": {
        "id": "eZgYmvwAIefG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# 필요한 모듈 임포트\"\"\"\n",
        "\n",
        "from google.colab import output as ot\n",
        "import torchtext\n",
        "import torchdata\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "nltk.download('all')\n",
        "ot.clear()\n",
        "\n",
        "\"\"\"# \b데이터셋 로드\"\"\"\n",
        "\n",
        "train_iter = torchtext.datasets.IMDB(split = 'train')\n",
        "test_iter = torchtext.datasets.IMDB(split = 'test')\n",
        "train_labels = []\n",
        "train_tokens = []\n",
        "\n",
        "test_labels = []\n",
        "test_tokens = []\n",
        "\n",
        "for label, line in tqdm(train_iter):\n",
        "  train_labels.append(label)\n",
        "  train_tokens.append(line)\n",
        "\n",
        "for label, line in tqdm(test_iter):\n",
        "  test_labels.append(label)\n",
        "  test_tokens.append(line)\n",
        "\n",
        "\n",
        "\"\"\" # 전처리 함수 \n",
        "\n",
        "1. HTML태그 제거\n",
        "2. 영어만 남기기\n",
        "3. 소문자 변환\n",
        "4. 불용어 제거\n",
        "5. 다시 하나의 문장으로 합쳐주기\n",
        "\"\"\"\n",
        "\n",
        "def processing(review):\n",
        "  review = BeautifulSoup(review, 'html5lib').get_text() # HTML 태그 제거\n",
        "  review = re.sub('[^a-zA-Z]', ' ', review) # 영어를 제외한 나머지 문자를 공백 치환\n",
        "  review = review.lower() # 소문자로 변환\n",
        "  review = review.split() # 공백을 기준으로 문장을 나눈다.(리스트 형태로)\n",
        "                                     # 여기서 공백이 몇개든 리스트 형태로 바뀌므로, 나중에 ' '.join(review)를 할때 깨끗하게 합쳐질 수 있는 것이다.\n",
        "  review = [r for r in review if not r in stopwords.words('english')] # 리스트 형태로 바꿨으니 안에 있는 단어 하나하나를 불용어 사전과 비교해서 불용어 제거한다.\n",
        "  review = ' '.join(review) # 리스트 안의 요소들을 공백을 구분자로 하여금 하나의 문장으로 합쳐준다.\n",
        "  return review\n",
        "\n",
        "\"\"\" # 전처리 함수 적용\"\"\"\n",
        "\n",
        "train_text = []\n",
        "test_text = []\n",
        "\n",
        "for r in tqdm(train_tokens):\n",
        "  r = processing(r)\n",
        "  train_text.append(r)\n",
        "\n",
        "for r in tqdm(test_tokens):\n",
        "  r = processing(r)\n",
        "  test_text.append(r)\n",
        "\n",
        "\n",
        "\"\"\" # 데이터셋을 단어 단위로 분할하고 이에 대한 동시 등장 행렬을 구하기 위해 다음과 같이 데이터셋을 전처리 한다.\"\"\"\n",
        "\n",
        "total_words = []\n",
        "for word_list in [w.split() for w in train_text]:\n",
        "  for word in word_list:\n",
        "    total_words.append(word)\n",
        "\n",
        "\"\"\" # 동시 등장 행렬을 구하고, glove를 학습시켜 준다.\"\"\"\n",
        "\n",
        "!pip install glove_python_binary # 글로브 다운로드\n",
        "from glove import Corpus, Glove\n",
        "corpus = Corpus() \n",
        "\n",
        "corpus.fit([total_words], window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiQpSMJjJFgU",
        "outputId": "281448f6-c8ed-4555-af66-7f42dc941ec6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "25000it [00:26, 938.46it/s] \n",
            "25000it [00:22, 1124.09it/s]\n",
            "100%|██████████| 25000/25000 [12:46<00:00, 32.63it/s]\n",
            "100%|██████████| 25000/25000 [11:39<00:00, 35.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: glove_python_binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.7.3)\n",
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 처음본 단어(UNK), 패딩(PAD) 토큰을 사전에 추가해주기\n",
        "\n",
        "처음본 단어는 UNK라는 토큰을 통해, 그리고 일정한 길이의 데이터 셋으로 맞추기 위해서 PAD라는 토큰을 glove 단어 사전에 추가해주어야 한다.\n",
        "\n",
        "참고로 glove.dictionary는 total_words에 등장한 순서대로(실제 학습 데이터에서 'rented'라는 단어가 제일 먼저 등장함) 인덱스를 부여하기 때문에 'rented'의 인덱스는 0이다.\n",
        "\n",
        "\n",
        "    glove.dictinonary['rented']\n",
        "    >> 0\n",
        "\n",
        "\n",
        "따라서 UNK라는 토큰을, 그리고 PAD라는 토큰을 차례대로 추가해준다."
      ],
      "metadata": {
        "id": "IW5acGihKuE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove.dictionary['<UNK>'] = len(glove.dictionary)\n",
        "glove.dictionary['<PAD>'] = len(glove.dictionary)"
      ],
      "metadata": {
        "id": "su__aebBLeLt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그 다음, word_vectors에서 각 토큰에 해당하는 인덱스가 입력되었을 때 **no_components**에 해당하는 차원을 가진 랜덤벡터(UNK)와 0(PAD)벡터를 출력할 수 있도록 추가해준다.\n",
        "\n",
        "나는 여기서 랜덤벡터를 가우시안 잡음으로 설정하였다."
      ],
      "metadata": {
        "id": "ral4oiR9Lx7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove.word_vectors = np.concatenate((glove.word_vectors, np.random.normal(size = (1, 100))), axis = 0)\n",
        "glove.word_vectors = np.concatenate((glove.word_vectors, np.zeros((1, 100))), axis = 0)\n",
        "glove.word_vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8JpMNmFMAGl",
        "outputId": "8173383f-f3fb-4352-8a43-7bb61bb1e9fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(74067, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리"
      ],
      "metadata": {
        "id": "DYQpn5I0SK9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" # 데이터 전처리 - 레이블 0과 1로 맞춰주기 \"\"\"\n",
        "train_y = np.array(train_labels) - 1\n",
        "test_y = np.array(test_labels) - 1\n",
        "\n",
        "\"\"\" # 데이터 전처리 - 문장 토큰화 \"\"\"\n",
        "train_tokens = np.array([t.split() for t in train_text])\n",
        "test_tokens = np.array([t.split() for t in test_text])\n",
        "\n",
        "\"\"\" # 데이터 전처리 - 토큰 정수화 \"\"\"\n",
        "def word_to_token(word):\n",
        "  try:\n",
        "    word_index = glove.dictionary[word]\n",
        "  except KeyError:\n",
        "    word_index = glove.dictionary['<UNK>']\n",
        "  return word_index\n",
        "\n",
        "train_indicies = []\n",
        "for sentence in train_tokens:\n",
        "  train_index = []\n",
        "  for word in sentence:\n",
        "    train_index.append(word_to_token(word))\n",
        "  train_indicies.append(train_index)\n",
        "\n",
        "test_indicies = []\n",
        "for sentence in test_tokens:\n",
        "  test_index = []\n",
        "  for word in sentence:\n",
        "    test_index.append(word_to_token(word))\n",
        "  test_indicies.append(test_index)\n",
        "\n",
        "\n",
        "\"\"\" # 데이터 전처리 - 정수화된 토큰의 길이를 맞춰주기 위해 패딩 추가 \"\"\"\n",
        "\n",
        "train_pad = pd.DataFrame(train_indicies).fillna(glove.dictionary['<PAD>'])\n",
        "test_pad = pd.DataFrame(test_indicies).fillna(glove.dictionary['<PAD>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmMtnMeDQljW",
        "outputId": "d1cf6744-cc37-4e43-b0f2-913c1084acfa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  import sys\n"
          ]
        }
      ]
    }
  ]
}